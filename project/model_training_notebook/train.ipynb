{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1550,
     "status": "ok",
     "timestamp": 1758773234048,
     "user": {
      "displayName": "Rohan S",
      "userId": "06766814340911486458"
     },
     "user_tz": -60
    },
    "id": "A6J0f5-yoA7x",
    "outputId": "64065b5f-8888-42cf-c2a4-fe71cb2afafb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/train\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1758773236235,
     "user": {
      "displayName": "Rohan S",
      "userId": "06766814340911486458"
     },
     "user_tz": -60
    },
    "id": "wnTAq8j5oIfD",
    "outputId": "6e0a6a5d-0464-4e3a-edc6-4e777c24de40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json  train.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4985748,
     "status": "ok",
     "timestamp": 1758778224319,
     "user": {
      "displayName": "Rohan S",
      "userId": "06766814340911486458"
     },
     "user_tz": -60
    },
    "id": "JOvMgKVWjJRz",
    "outputId": "4c0b8425-6e2d-40a8-b014-18ccb4edd612"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Plant Disease Detection System\n",
    "# ==============================\n",
    "\n",
    "# -------------------------\n",
    "# Imports\n",
    "# -------------------------\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# -------------------------\n",
    "# Global Params\n",
    "# -------------------------\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "BASE_DIR = \"plantvillage dataset/color\"\n",
    "MODEL_PATH = \"plant_disease_prediction_model.h5\"\n",
    "\n",
    "# -------------------------\n",
    "# Kaggle Dataset Download\n",
    "# -------------------------\n",
    "def download_and_extract_dataset(dataset_name=\"abdallahalidev/plantvillage-dataset\", zip_name=\"plantvillage-dataset.zip\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts a dataset from Kaggle.\n",
    "    Requires kaggle.json file with credentials in the working directory.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    from zipfile import ZipFile\n",
    "\n",
    "    # Load Kaggle credentials\n",
    "    kaggle_credentials = json.load(open(\"kaggle.json\"))\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = kaggle_credentials[\"username\"]\n",
    "    os.environ[\"KAGGLE_KEY\"] = kaggle_credentials[\"key\"]\n",
    "\n",
    "    # Download dataset\n",
    "    if not os.path.exists(zip_name):\n",
    "        print(\"Downloading dataset from Kaggle...\")\n",
    "        subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_name, \"-p\", \".\"])\n",
    "\n",
    "    # Extract dataset\n",
    "    if not os.path.exists(\"plantvillage dataset\"):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with ZipFile(zip_name, \"r\") as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "        print(\"Extraction complete.\")\n",
    "\n",
    "# -------------------------\n",
    "# Data Preprocessing\n",
    "# -------------------------\n",
    "def create_data_generators(base_dir=BASE_DIR, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Creates train and validation data generators with augmentation & normalization.\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_gen = datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    val_gen = datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen\n",
    "\n",
    "# -------------------------\n",
    "# Model Definition\n",
    "# -------------------------\n",
    "def build_model(num_classes, img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Builds a pretrained EfficientNetB0 model with dropout layers.\n",
    "    \"\"\"\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size, 3))\n",
    "    base_model.trainable = False  # Freeze base\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Class Weights for Imbalance\n",
    "# -------------------------\n",
    "def compute_class_weights(generator):\n",
    "    \"\"\"\n",
    "    Compute class weights to handle imbalance.\n",
    "    \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    classes = generator.classes\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(classes), y=classes)\n",
    "    return dict(enumerate(class_weights))\n",
    "\n",
    "# -------------------------\n",
    "# Training Pipeline\n",
    "# -------------------------\n",
    "def train_model(model, train_gen, val_gen, epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Trains the model with callbacks for early stopping, LR scheduler, and checkpoints.\n",
    "    \"\"\"\n",
    "    cb = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2, min_lr=1e-6),\n",
    "        callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True),\n",
    "        callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        class_weight=compute_class_weights(train_gen),\n",
    "        callbacks=cb\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "def evaluate_model(model, val_gen):\n",
    "    \"\"\"\n",
    "    Evaluate model with classification report & confusion matrix.\n",
    "    \"\"\"\n",
    "    val_gen.reset()\n",
    "    preds = model.predict(val_gen, verbose=1)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    y_true = val_gen.classes\n",
    "    class_labels = list(val_gen.class_indices.keys())\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Grad-CAM Visualization\n",
    "# -------------------------\n",
    "def grad_cam(model, img_path, layer_name=None):\n",
    "    \"\"\"\n",
    "    Visualize important regions in the image using Grad-CAM.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).resize((IMG_SIZE, IMG_SIZE))\n",
    "    img_array = np.expand_dims(np.array(img) / 255.0, axis=0)\n",
    "\n",
    "    if layer_name is None:\n",
    "        layer_name = [layer.name for layer in model.layers if \"conv\" in layer.name][-1]\n",
    "\n",
    "    grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(img_array)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_output)[0]\n",
    "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    cam = tf.reduce_sum(tf.multiply(weights, conv_output[0]), axis=-1)\n",
    "\n",
    "    cam = np.maximum(cam, 0) / np.max(cam)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(cam, cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(\"Grad-CAM\")\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Main Execution\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Download dataset\n",
    "    download_and_extract_dataset()\n",
    "\n",
    "    # Step 2: Create data generators\n",
    "    train_gen, val_gen = create_data_generators()\n",
    "\n",
    "    # Step 3: Build and train model\n",
    "    model = build_model(num_classes=train_gen.num_classes)\n",
    "    history = train_model(model, train_gen, val_gen)\n",
    "\n",
    "    # Step 4: Save final model and class indices\n",
    "    model.save(MODEL_PATH)\n",
    "    json.dump(train_gen.class_indices, open(\"class_indices.json\", \"w\"))\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    evaluate_model(model, val_gen)\n",
    "\n",
    "    # Optional: Grad-CAM visualization\n",
    "    # grad_cam(model, \"sample_leaf.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOP2xqthNueAULYcFNYfhC9",
   "gpuType": "T4",
   "mount_file_id": "1RVQX7rjCXMdXRiul8bonvhHWFUbBB406",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
